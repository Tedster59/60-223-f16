#N canvas 209 30 762 655 10;
#X obj -166 -214 cnv 15 600 100 empty empty 2.a.iii.3_gesture-recognition
20 12 0 14 -204800 -66577 0;
#X obj 34 -104 cnv 15 400 60 empty empty empty 20 12 0 14 -262130 -66577
0;
#X text -122 18 from the "Control" app;
#X floatatom -54 115 5 0 0 0 - - -, f 5;
#X floatatom -16 115 5 0 0 0 - - -, f 5;
#X floatatom 22 115 5 0 0 0 - - -, f 5;
#X text -40 93 x;
#X text 3 94 y;
#X text 41 94 z;
#X obj -56 66 unpack 1 1 1;
#N canvas 842 57 1023 943 training-and-mapping 0;
#X obj 131 643 outlet;
#X msg 540 277 63.2421 64.5294 35.9906;
#X text 512 303 accelerometer data from mobile phone;
#X obj 131 396 list prepend map;
#X obj 538 180 list prepend set;
#X obj 214 225 spigot;
#X obj 156 225 spigot;
#X obj 538 224 list trim;
#X obj 131 451 list trim;
#X obj 5 504 list trim;
#X obj 74 227 + 1;
#X obj 208 272 bng 15 250 50 0 empty empty empty 17 7 0 10 -262144
-1 -1;
#X obj 273 271 bng 15 250 50 0 empty empty empty 17 7 0 10 -262144
-1 -1;
#X obj 538 143 inlet;
#X obj 217 50 inlet;
#X obj -25 96 inlet;
#X obj 112 51 inlet;
#X obj 5 452 list prepend add;
#X obj 5 396 list prepend;
#X obj 112 101 t float float;
#X msg 312 234 record \$1;
#X obj 217 102 t float float;
#X connect 3 0 8 0;
#X connect 4 0 7 0;
#X connect 5 0 3 0;
#X connect 5 0 12 0;
#X connect 6 0 11 0;
#X connect 6 0 18 0;
#X connect 7 0 1 0;
#X connect 8 0 0 0;
#X connect 9 0 0 0;
#X connect 10 0 18 1;
#X connect 13 0 4 0;
#X connect 13 0 5 0;
#X connect 13 0 6 0;
#X connect 14 0 21 0;
#X connect 15 0 10 0;
#X connect 16 0 19 0;
#X connect 17 0 9 0;
#X connect 18 0 17 0;
#X connect 19 0 6 1;
#X connect 19 1 20 0;
#X connect 20 0 0 0;
#X connect 21 0 5 1;
#X connect 21 1 20 0;
#X restore -59 396 pd training-and-mapping;
#X msg -103 208 clear;
#X msg -67 265 train;
#X text -76 242 4 train;
#X obj 57 209 hradio 15 1 0 3 empty empty empty 0 -8 0 10 -262144 -1
-1 0;
#X text -137 176 1 clear the model;
#X floatatom -135 477 5 0 0 0 - - -, f 5;
#X text -82 476 classifcation;
#X text -129 -182 Using Dynamic Time Warping to recognize gestures
\, using sensor data from a mobile phone;
#X text 61 -97 With gesture data from a mobile phone streaming into
PureData wirelessly using OpenSoundControl \, we use Dynamic Time Warping
to recognize several different gestures;
#X obj -137 423 ml.dtw;
#X obj 56 265 tgl 50 0 empty empty learn 17 7 0 10 -262144 -1 -1 0
1;
#X obj 116 265 tgl 50 0 empty empty classify 17 7 0 10 -262144 -1 -1
0 1;
#X text 10 176 2 choose what gesture number you're training;
#X text 210 207 each of the 3 squares corresponds with a festure "class"
in order to train \, select the class number by clicking on one of
the three squares \, then turn ON "learn" and perform the gesture (e.g.
move mobile phone from vertical to horizental orientation) \, then
turn OFF "learn". reapeat these steps for two more gestures. then click
on "train". in order to classify \, click on "classify" \, then perform
gesture" and watch tht output of "ml.dtw" below. turn OFF "classify"
at the end of each gesture. repeat for classifying other gestures.
;
#X text 106 209 <---------------;
#X obj -160 -94 bng 15 250 50 0 empty empty empty 17 7 0 10 -262144
-1 -1;
#X obj -136 -95 udpreceive 10000;
#X obj -136 -62 unpackOSC;
#X obj -136 -2 routeOSC /accelerometer /gryo;
#X text 58 247 3 learn;
#X text 211 371 Note: increase the update rate in your mobile apps
to get better classification results \, especially when your gestures
are fast;
#X connect 9 0 3 0;
#X connect 9 1 4 0;
#X connect 9 2 5 0;
#X connect 10 0 20 0;
#X connect 11 0 20 0;
#X connect 12 0 20 0;
#X connect 14 0 10 0;
#X connect 20 0 16 0;
#X connect 21 0 10 1;
#X connect 22 0 10 2;
#X connect 27 0 28 0;
#X connect 27 0 26 0;
#X connect 28 0 29 0;
#X connect 29 0 9 0;
#X connect 29 0 10 3;
